<?xml version="1.0" encoding="UTF-8"?>
<html xml:lang="en-us" lang="en-us" xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops" xmlns:ns="http://www.w3.org/2001/10/synthesis">
<head>
<title>The Linux Programming Interface</title>
<link rel="stylesheet" type="text/css" href="../styles/9781593272203.css"/>
</head>
<body>
<h2 class="h2" id="ch60"><span epub:type="pagebreak" id="page_1239"/><strong><span class="big">60</span></strong><br/><strong>SOCKETS: SERVER DESIGN</strong></h2>
<p class="noindenta">This chapter discusses the fundamentals of designing iterative and concurrent servers and describes <em>inetd</em>, a special daemon designed to facilitate the creation of Internet servers.</p>
<h3 class="h3" id="ch60lev1sec01"><strong>60.1 Iterative and Concurrent Servers</strong></h3>
<p class="noindentab">Two common designs for network servers using sockets are the following:</p>
<p class="bull">&#8226; <em>Iterative</em>: The server handles one client at a time, processing that client&#8217;s request(s) completely, before proceeding to the next client.</p>
<p class="bull">&#8226; <em>Concurrent</em>: The server is designed to handle multiple clients simultaneously.</p>
<p class="noindentt">We have already seen an example of an iterative server using FIFOs in <a href="ch44.xhtml#ch44lev1sec08">Section 44.8</a> and an example of a concurrent server using System V message queues in <a href="ch46.xhtml#ch46lev1sec08">Section 46.8</a>.</p>
<p class="indent">Iterative servers are usually suitable only when client requests can be handled quickly, since each client must wait until all of the preceding clients have been serviced. A typical scenario for employing an iterative server is where the client and server exchange a single request and response.</p>
<p class="indent"><span epub:type="pagebreak" id="page_1240"/>Concurrent servers are suitable when a significant amount of processing time is required to handle each request, or where the client and server engage in an extended conversation, passing messages back and forth. In this chapter, we mainly focus on the traditional (and simplest) method of designing a concurrent server: creating a new child process for each new client. Each server child performs all tasks necessary to service a single client and then terminates. Since each of these processes can operate independently, multiple clients can be handled simultaneously. The principal task of the main server process (the parent) is to create a new child process for each new client. (A variation on this approach is to create a new thread for each client.)</p>
<p class="indent">In the following sections, we look at examples of an iterative and a concurrent server using Internet domain sockets. These two servers implement the <em>echo</em> service (RFC 862), a rudimentary service that returns a copy of whatever the client sends it.</p>
<h3 class="h3" id="ch60lev1sec02"><strong>60.2 An Iterative UDP <em>echo</em> Server</strong></h3>
<p class="noindenta">In this and the next section, we present servers for the <em>echo</em> service. The <em>echo</em> service operates on both UDP and TCP port 7. (Since this is a reserved port, the <em>echo</em> server must be run with superuser privileges.)</p>
<p class="indent">The UDP <em>echo</em> server continuously reads datagrams, returning a copy of each datagram to the sender. Since the server needs to handle only a single message at a time, an iterative server design suffices. The header file for the server (and the client that we discuss in a moment) is shown in <a href="ch60.xhtml#ch60ex1">Listing 60-1</a>.</p>
<p class="examplet"><a id="ch60ex1"/><strong>Listing 60-1:</strong> Header file for <span class="literal">id_echo_sv.c</span> and <span class="literal">id_echo_cl.c</span></p>
<p class="programsli">________________________________________________________ <span class="codestrong">sockets/id_echo.h</span><br/><br/>#include "inet_sockets.h"&#160;&#160;&#160;&#160;&#160;/* Declares our socket functions */<br/>#include "tlpi_hdr.h"<br/><br/>#define SERVICE "echo"&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Name of UDP service */<br/><br/>#define BUF_SIZE 500&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Maximum size of datagrams that can<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;be read by client and server */<br/>________________________________________________________ <span class="codestrong">sockets/id_echo.h</span></p>
<p class="noindentb"><a href="ch60.xhtml#ch60ex2">Listing 60-2</a> shows the implementation of the server. Note the following points regarding the server implementation:</p>
<p class="bull">&#8226; We use the <em>becomeDaemon()</em> function of <a href="ch37.xhtml#ch37lev1sec02">Section 37.2</a> to turn the server into a daemon.</p>
<p class="bull">&#8226; To shorten this program, we employ the Internet domain sockets library developed in <a href="ch59.xhtml#ch59lev1sec12">Section 59.12</a>.</p>
<p class="bull">&#8226; If the server can&#8217;t send a reply to the client, it logs a message using <em>syslog()</em>.</p>
<div class="block1">
<p class="noindent">In a real-world application, we would probably apply some rate limit to the messages written with <em>syslog()</em>, both to prevent the possibility of an attacker filling the system log and because each call to <em>syslog()</em> is expensive, since (by default) <em>syslog()</em> in turn calls <em>fsync()</em>.</p>
</div>
<p class="examplet"><span epub:type="pagebreak" id="page_1241"/><a id="ch60ex2"/><strong>Listing 60-2:</strong> An iterative server that implements the UDP <em>echo</em> service</p>
<p class="programsli">_____________________________________________________ <span class="codestrong">sockets/id_echo_sv.c</span><br/><br/>#include &lt;syslog.h&gt;<br/>#include "id_echo.h"<br/>#include "become_daemon.h"<br/><br/>int<br/>main(int argc, char *argv[])<br/>{<br/>&#160;&#160;&#160;&#160;int sfd;<br/>&#160;&#160;&#160;&#160;ssize_t numRead;<br/>&#160;&#160;&#160;&#160;socklen_t len;<br/>&#160;&#160;&#160;&#160;struct sockaddr_storage claddr;<br/>&#160;&#160;&#160;&#160;char buf[BUF_SIZE];<br/>&#160;&#160;&#160;&#160;char addrStr[IS_ADDR_STR_LEN];<br/><br/>&#160;&#160;&#160;&#160;if (becomeDaemon(0) == -1)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;errExit("becomeDaemon");<br/><br/>&#160;&#160;&#160;&#160;sfd = inetBind(SERVICE, SOCK_DGRAM, NULL);<br/>&#160;&#160;&#160;&#160;if (sfd == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Could not create server socket (%s)", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;/* Receive datagrams and return copies to senders */<br/><br/>&#160;&#160;&#160;&#160;for (;;) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;len = sizeof(struct sockaddr_storage);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;numRead = recvfrom(sfd, buf, BUF_SIZE, 0,<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;(struct sockaddr *) &#38;claddr, &#38;len);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (numRead == -1)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;errExit("recvfrom");<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (sendto(sfd, buf, numRead, 0, (struct sockaddr *) &#38;claddr, len)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;!= numRead)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_WARNING, "Error echoing response to %s (%s)",<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;inetAddressStr((struct sockaddr *) &#38;claddr, len,<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;addrStr, IS_ADDR_STR_LEN),<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;strerror(errno));<br/>&#160;&#160;&#160;&#160;}<br/>}<br/>_____________________________________________________ <span class="codestrong">sockets/id_echo_sv.c</span></p>
<p class="noindent">To test the server, we use the client program shown in <a href="ch60.xhtml#ch60ex3">Listing 60-3</a>. This program also employs the Internet domain sockets library developed in <a href="ch59.xhtml#ch59lev1sec12">Section 59.12</a>. As its first command-line argument, the client program expects the name of the host on which the server resides. The client executes a loop in which it sends each of its remaining command-line arguments to the server as separate datagrams, and reads and prints each response datagram sent back by the server.</p>
<p class="examplet"><span epub:type="pagebreak" id="page_1242"/><a id="ch60ex3"/><strong>Listing 60-3:</strong> A client for the UDP <em>echo</em> service</p>
<p class="programsli">_____________________________________________________ <span class="codestrong">sockets/id_echo_cl.c</span><br/><br/>#include "id_echo.h"<br/><br/>int<br/>main(int argc, char *argv[])<br/>{<br/>&#160;&#160;&#160;&#160;int sfd, j;<br/>&#160;&#160;&#160;&#160;size_t len;<br/>&#160;&#160;&#160;&#160;ssize_t numRead;<br/>&#160;&#160;&#160;&#160;char buf[BUF_SIZE];<br/><br/>&#160;&#160;&#160;&#160;if (argc &lt; 2 || strcmp(argv[1], "--help") == 0)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;usageErr("%s host msg...\n", argv[0]);<br/><br/>&#160;&#160;&#160;&#160;/* Construct server address from first command-line argument */<br/><br/>&#160;&#160;&#160;&#160;sfd = inetConnect(argv[1], SERVICE, SOCK_DGRAM);<br/>&#160;&#160;&#160;&#160;if (sfd == -1)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;fatal("Could not connect to server socket");<br/><br/>&#160;&#160;&#160;&#160;/* Send remaining command-line arguments to server as separate datagrams */<br/><br/>&#160;&#160;&#160;&#160;for (j = 2; j &lt; argc; j++) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;len = strlen(argv[j]);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (write(sfd, argv[j], len) != len)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;fatal("partial/failed write");<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;numRead = read(sfd, buf, BUF_SIZE);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (numRead == -1)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;errExit("read");<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;printf("[%ld bytes] %.*s\n", (long) numRead, (int) numRead, buf);<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;exit(EXIT_SUCCESS);<br/>}<br/>_____________________________________________________ <span class="codestrong">sockets/id_echo_cl.c</span></p>
<p class="noindent">Here is an example of what we see when we run the server and two instances of the client:</p>
<p class="programs">$ <span class="codestrong">su</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<span class="font1">Need privilege to bind reserved port</span><br/>Password:<br/># <span class="codestrong">./id_echo_sv</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<span class="font1">Server places itself in background</span><br/># <span class="codestrong">exit</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<span class="font1">Cease to be superuser</span><br/>$ <span class="codestrong">./id_echo_cl localhost hello world</span>&#160;&#160;&#160;&#160;&#160;<span class="font1">This client sends two datagrams</span><br/>[5 bytes] hello&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<span class="font1">Client prints responses from server</span><br/>[5 bytes] world<br/>$ <span class="codestrong">./id_echo_cl localhost goodbye</span>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;<span class="font1">This client sends one datagram</span><br/>[7 bytes] goodbye</p>
<h3 class="h3" id="ch60lev1sec03"><span epub:type="pagebreak" id="page_1243"/><strong>60.3 A Concurrent TCP <em>echo</em> Server</strong></h3>
<p class="noindenta">The TCP <em>echo</em> service also operates on port 7. The TCP <em>echo</em> server accepts a connection and then loops continuously, reading all transmitted data and sending it back to the client on the same socket. The server continues reading until it detects end-of-file, at which point it closes its socket (so that the client sees end-of-file if it is still reading from its socket).</p>
<p class="indentb">Since the client may send an indefinite amount of data to the server (and thus servicing the client may take an indefinite amount of time), a concurrent server design is appropriate, so that multiple clients can be simultaneously served. The server implementation is shown in <a href="ch60.xhtml#ch60ex4">Listing 60-4</a>. (We show an implementation of a client for this service in <a href="ch61.xhtml#ch61lev1sec02">Section 61.2</a>.) Note the following points about the implementation:</p>
<p class="bull">&#8226; The server becomes a daemon by calling the <em>becomeDaemon()</em> function shown in <a href="ch37.xhtml#ch37lev1sec02">Section 37.2</a>.</p>
<p class="bull">&#8226; To shorten this program, we employ the Internet domain sockets library shown in <a href="ch59.xhtml#ch59ex9">Listing 59-9</a> (<a href="ch59.xhtml#page_1228">page 1228</a>).</p>
<p class="bull">&#8226; Since the server creates a child process for each client connection, we must ensure that zombies are reaped. We do this within a <span class="literal">SIGCHLD</span> handler.</p>
<p class="bull">&#8226; The main body of the server consists of a <span class="literal">for</span> loop that accepts a client connection and then uses <em>fork()</em> to create a child process that invokes the <em>handleRequest()</em> function to handle that client. In the meantime, the parent continues around the <span class="literal">for</span> loop to accept the next client connection.</p>
<div class="block">
<p class="noindent">In a real-world application, we would probably include some code in our server to place an upper limit on the number of child processes that the server could create, in order to prevent an attacker from attempting a remote fork bomb by using the service to create so many processes on the system that it becomes unusable. We could impose this limit by adding extra code in the server to count the number of children currently executing (this count would be incremented after a successful <em>fork()</em> and decremented as each child was reaped in the <span class="literal">SIGCHLD</span> handler). If the limit on the number of children were reached, we could then temporarily stop accepting connections (or alternatively, accept connections and then immediately close them).</p>
</div>
<p class="bull">&#8226; After each <em>fork()</em>, the file descriptors for the listening and connected sockets are duplicated in the child (<a href="ch24.xhtml#ch24lev2sec01">Section 24.2.1</a>). This means that both the parent and the child could communicate with the client using the connected socket. However, only the child needs to perform such communication, and so the parent closes the file descriptor for the connected socket immediately after the <em>fork()</em>. (If the parent did not do this, then the socket would never actually be closed; furthermore, the parent would eventually run out of file descriptors.) Since the child doesn&#8217;t accept new connections, it closes its duplicate of the file descriptor for the listening socket.</p>
<p class="bull">&#8226; Each child process terminates after handling a single client.</p>
<p class="examplet"><span epub:type="pagebreak" id="page_1244"/><a id="ch60ex4"/><strong>Listing 60-4:</strong> A concurrent server that implements the TCP <em>echo</em> service</p>
<p class="programsli">_____________________________________________________ <span class="codestrong">sockets/is_echo_sv.c</span><br/><br/>#include &lt;signal.h&gt;<br/>#include &lt;syslog.h&gt;<br/>#include &lt;sys/wait.h&gt;<br/>#include "become_daemon.h"<br/>#include "inet_sockets.h"&#160;&#160;&#160;&#160;&#160;&#160;/* Declarations of inet*() socket functions */<br/>#include "tlpi_hdr.h"<br/><br/>#define SERVICE "echo"&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Name of TCP service */<br/>#define BUF_SIZE 4096<br/><br/>static void&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* SIGCHLD handler to reap dead child processes */<br/>grimReaper(int sig)<br/>{<br/>&#160;&#160;&#160;&#160;int savedErrno;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Save 'errno' in case changed here */<br/><br/>&#160;&#160;&#160;&#160;savedErrno = errno;<br/>&#160;&#160;&#160;&#160;while (waitpid(-1, NULL, WNOHANG) &gt; 0)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;continue;<br/>&#160;&#160;&#160;&#160;errno = savedErrno;<br/>}<br/><br/>/* Handle a client request: copy socket input back to socket */<br/><br/>static void<br/>handleRequest(int cfd)<br/>{<br/>&#160;&#160;&#160;&#160;char buf[BUF_SIZE];<br/>&#160;&#160;&#160;&#160;ssize_t numRead;<br/><br/>&#160;&#160;&#160;&#160;while ((numRead = read(cfd, buf, BUF_SIZE)) &gt; 0) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (write(cfd, buf, numRead) != numRead) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "write() failed: %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;if (numRead == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Error from read(): %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;}<br/>}<br/><br/>int<br/>main(int argc, char *argv[])<br/>{<br/>&#160;&#160;&#160;&#160;int lfd, cfd;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Listening and connected sockets */<br/>&#160;&#160;&#160;&#160;struct sigaction sa;<br/><br/>&#160;&#160;&#160;&#160;if (becomeDaemon(0) == -1)<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;errExit("becomeDaemon");<br/>&#160;&#160;&#160;&#160;sigemptyset(&#38;sa.sa_mask);<br/>&#160;&#160;&#160;&#160;sa.sa_flags = SA_RESTART;<br/>&#160;&#160;&#160;&#160;sa.sa_handler = grimReaper;<br/>&#160;&#160;&#160;&#160;if (sigaction(SIGCHLD, &#38;sa, NULL) == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Error from sigaction(): %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;lfd = inetListen(SERVICE, 10, NULL);<br/>&#160;&#160;&#160;&#160;if (lfd == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Could not create server socket (%s)", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;for (;;) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;cfd = accept(lfd, NULL, NULL);&#160;&#160;/* Wait for connection */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (cfd == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Failure in accept(): %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Handle each client request in a new child process */<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;switch (fork()) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;case -1:<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Can't create child (%s)", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;close(cfd);&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Give up on this client */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;break;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* May be temporary; try next client */<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;case 0:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Child */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;close(lfd);&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Unneeded copy of listening socket */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;handleRequest(cfd);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;_exit(EXIT_SUCCESS);<br/><br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;default:&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Parent */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;close(cfd);&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Unneeded copy of connected socket */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;break;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;/* Loop to accept next connection */<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}<br/>&#160;&#160;&#160;&#160;}<br/>}<br/>_____________________________________________________ <span class="codestrong">sockets/is_echo_sv.c</span></p>
<h3 class="h3" id="ch60lev1sec04"><span epub:type="pagebreak" id="page_1245"/><strong>60.4 Other Concurrent Server Designs</strong></h3>
<p class="noindenta">The traditional concurrent server model described in the previous section is adequate for many applications that need to simultaneously handle multiple clients via TCP connections. However, for very high-load servers (for example, web servers handling thousands of requests per minute), the cost of creating a new child (or even thread) for each client imposes a significant burden on the server (refer to <a href="ch28.xhtml#ch28lev1sec03">Section 28.3</a>), and alternative designs need to be employed. We briefly consider some of these alternatives.</p>
<h5 class="h5" id="ch60lev3sec01"><span epub:type="pagebreak" id="page_1246"/><strong>Preforked and prethreaded servers</strong></h5>
<p class="noindentab">Preforked and prethreaded servers are described in some detail in <a href="ch30.xhtml#ch30">Chapter 30</a> of [<a href="bib.xhtml#bib93">Stevens et al., 2004</a>]. The key ideas are the following:</p>
<p class="bull">&#8226; Instead of creating a new child process (or thread) for each client, the server precreates a fixed number of child processes (or threads) immediately on startup (i.e., before any client requests are even received). These children constitute a so-called <em>server pool</em>.</p>
<p class="bull">&#8226; Each child in the server pool handles one client at a time, but instead of terminating after handling the client, the child fetches the next client to be serviced and services it, and so on.</p>
<p class="noindentt">Employing the above technique requires some careful management within the server application. The server pool should be large enough to ensure adequate response to client requests. This means that the server parent must monitor the number of unoccupied children, and, in times of peak load, increase the size of the pool so that there are always enough child processes available to immediately serve new clients. If the load decreases, then the size of the server pool should be reduced, since having excess processes on the system can degrade overall system performance.</p>
<p class="indent">In addition, the children in the server pool must follow some protocol to allow them to exclusively select individual client connections. On most UNIX implementations (including Linux), it is sufficient to have each child in the pool block in an <em>accept()</em> call on the listening descriptor. In other words, the server parent creates the listening socket before creating any children, and each of the children inherits a file descriptor for the socket during the <em>fork()</em>. When a new client connection arrives, only one of the children will complete the <em>accept()</em> call. However, because <em>accept()</em> is not an atomic system call on some older implementations, the call may need to be bracketed by some mutual-exclusion technique (e.g., a file lock) to ensure that only one child at a time performs the call ([<a href="bib.xhtml#bib93">Stevens et al., 2004</a>]).</p>
<div class="block">
<p class="noindent">There are alternatives to having all of the children in the server pool perform <em>accept()</em> calls. If the server pool consists of separate processes, the server parent can perform the <em>accept()</em> call, and then pass the file descriptor containing the new connection to one of the free processes in the pool, using a technique that we briefly describe in <a href="ch61.xhtml#ch61lev2sec10">Section 61.13.3</a>. If the server pool consists of threads, the main thread can perform the <em>accept()</em> call, and then inform one of the free server threads that a new client is available on the connected descriptor.</p>
</div>
<h5 class="h5" id="ch60lev3sec02"><strong>Handling multiple clients from a single process</strong></h5>
<p class="noindenta">In some cases, we can design a single server process to handle multiple clients. To do this, we must employ one of the I/O models (I/O multiplexing, signal-driven I/O, or <em>epoll</em>) that allow a single process to simultaneously monitor multiple file descriptors for I/O events. These models are described in <a href="ch63.xhtml#ch63">Chapter 63</a>.</p>
<p class="indent">In a single-server design, the server process must take on some of the scheduling tasks that are normally handled by the kernel. In a solution that involves one server process per client, we can rely on the kernel to ensure that each server process (and thus client) gets a fair share of access to the resources of the server host. But when we use a single server process to handle multiple clients, the server must do some work <span epub:type="pagebreak" id="page_1247"/>to ensure that one or a few clients don&#8217;t monopolize access to the server while other clients are starved. We say a little more about this point in <a href="ch63.xhtml#ch63lev2sec15">Section 63.4.6</a>.</p>
<h5 class="h5" id="ch60lev3sec03"><strong>Using server farms</strong></h5>
<p class="noindenta">Other approaches to handling high client loads involve the use of multiple server systems&#8212;a <em>server farm</em>.</p>
<p class="indent">One of the simplest approaches to building a server farm (employed by some web servers) is <em>DNS round-robin load sharing</em> (or <em>load distribution</em>), where the authoritative name server for a zone maps the same domain name to several IP addresses (i.e., several servers share the same domain name). Successive requests to the DNS server to resolve the domain name return these IP addresses in round-robin order. Further information about DNS round-robin load sharing can be found in [<a href="bib.xhtml#bib02">Albitz &#38; Liu, 2006</a>].</p>
<p class="indent">Round-robin DNS has the advantage of being inexpensive and easy to set up. However, it does have some shortcomings. A DNS server performing iterative resolution may cache its results (see <a href="ch59.xhtml#ch59lev1sec08">Section 59.8</a>), with the result that future queries on the domain name return the same IP address, instead of the round-robin sequence generated by the authoritative DNS server. Also, round-robin DNS doesn&#8217;t have any built-in mechanisms for ensuring good load balancing (different clients may place different loads on a server) or ensuring high availability (what if one of the servers dies or the server application that it is running crashes?). Another issue that we may need to consider&#8212;one that is faced by many designs that employ multiple server machines&#8212;is ensuring <em>server affinity</em>; that is, ensuring that a sequence of requests from the same client are all directed to the same server, so that any state information maintained by the server about the client remains accurate.</p>
<p class="indent">A more flexible, but also more complex, solution is <em>server load balancing</em>. In this scenario, a single load-balancing server routes incoming client requests to one of the members of the server farm. (To ensure high availability, there may be a backup server that takes over if the primary load-balancing server crashes.) This eliminates the problems associated with remote DNS caching, since the server farm presents a single IP address (that of the load-balancing server) to the outside world. The load-balancing server incorporates algorithms to measure or estimate server load (perhaps based on metrics supplied by the members of the server farm) and intelligently distribute the load across the members of the server farm. The load-balancing server also automatically detects failures in members of the server farm (and the addition of new servers, if demand requires it). Finally, a load-balancing server may also provide support for server affinity. Further information about server load balancing can be found in [<a href="bib.xhtml#bib48">Kopparapu, 2002</a>].</p>
<h3 class="h3" id="ch60lev1sec05"><strong>60.5 The <em>inetd</em> (Internet Superserver) Daemon</strong></h3>
<p class="noindenta">If we look through the contents of <span class="literal">/etc/services</span>, we see literally hundreds of different services listed. This implies that a system could theoretically be running a large number of server processes. However, most of these servers would usually be doing nothing but waiting for infrequent connection requests or datagrams. All of these server processes would nevertheless occupy slots in the kernel process table, and consume some memory and swap space, thus placing a load on the system.</p>
<p class="indentb"><span epub:type="pagebreak" id="page_1248"/>The <em>inetd</em> daemon is designed to eliminate the need to run large numbers of infrequently used servers. Using <em>inetd</em> provides two main benefits:</p>
<p class="bull">&#8226; Instead of running a separate daemon for each service, a single process&#8212;the <em>inetd</em> daemon&#8212;monitors a specified set of socket ports and starts other servers as required. Thus, the number of processes running on the system is reduced.</p>
<p class="bull">&#8226; The programming of the servers started by <em>inetd</em> is simplified, because <em>inetd</em> performs several of the steps that are commonly required by all network servers on startup.</p>
<p class="noindentt">Since it oversees a range of services, invoking other servers as required, <em>inetd</em> is sometimes known as the <em>Internet superserver</em>.</p>
<div class="block">
<p class="noindent">An extended version of <em>inetd</em>, <em>xinetd</em>, is provided in some Linux distributions. Among other things, <em>xinetd</em> adds a number of security enhancements. Information about <em>xinetd</em> can be found at <em><a href="http://www.xinetd.org/">http://www.xinetd.org/</a></em>.</p>
</div>
<h5 class="h5" id="ch60lev3sec04"><strong>Operation of the <em>inetd</em> daemon</strong></h5>
<p class="noindenta">The <em>inetd</em> daemon is normally started during system boot. After becoming a daemon process (<a href="ch37.xhtml#ch37lev1sec02">Section 37.2</a>), <em>inetd</em> performs the following steps:</p>
<ol>
<li class="order"><p class="orderp">For each of the services specified in its configuration file, <span class="literal">/etc/inetd.conf</span>, <em>inetd</em> creates a socket of the appropriate type (i.e., stream or datagram) and binds it to the specified port. Each TCP socket is additionally marked to permit incoming connections via a call to <em>listen()</em>.</p></li>
<li class="order"><p class="orderp">Using the <em>select()</em> system call (<a href="ch63.xhtml#ch63lev2sec03">Section 63.2.1</a>), <em>inetd</em> monitors all of the sockets created in the preceding step for datagrams or incoming connection requests.</p></li>
<li class="order"><p class="orderp">The <em>select()</em> call blocks until either a UDP socket has a datagram available to read or a connection request is received on a TCP socket. In the case of a TCP connection, <em>inetd</em> performs an <em>accept()</em> for the connection before proceeding to the next step.</p></li>
<li class="order"><p class="orderp">To start the server specified for this socket, <em>inetd()</em> calls <em>fork()</em> to create a new process that then does an <em>exec()</em> to start the server program. Before performing the <em>exec()</em>, the child process performs the following steps:</p>
<p class="olista">a) Close all of the file descriptors inherited from its parent, except the one for the socket on which the UDP datagram is available or the TCP connection has been accepted.</p>
<p class="olista">b) Use the techniques described in <a href="ch05.xhtml#ch05lev1sec05">Section 5.5</a> to duplicate the socket file descriptor on file descriptors 0, 1, and 2, and close the socket file descriptor itself (since it is no longer required). After this step, the execed server is able to communicate on the socket by using the three standard file descriptors.</p>
<p class="olista">c) Optionally, set the user and group IDs for the execed server to values specified in <span class="literal">/etc/inetd.conf</span>.</p></li>
<li class="order"><p class="orderp">If a connection was accepted on a TCP socket in step 3, <em>inetd</em> closes the connected socket (since it is needed only in the execed server).</p></li>
<li class="order"><p class="orderp">The <em>inetd</em> server returns to step 2.</p></li>
</ol>
<h5 class="h5" id="ch60lev3sec05"><span epub:type="pagebreak" id="page_1249"/><strong>The</strong> <span class="literal"><span class="codestrong">/etc/inetd.conf</span></span> <strong>file</strong></h5>
<p class="noindenta">The operation of the <em>inetd</em> daemon is controlled by a configuration file, normally <span class="literal">/etc/inetd.conf</span>. Each line in this file describes one of the services to be handled by <em>inetd</em>. <a href="ch60.xhtml#ch60ex5">Listing 60-5</a> shows some examples of entries in the <span class="literal">/etc/inetd.conf</span> file that comes with one Linux distribution.</p>
<p class="examplet"><a id="ch60ex5"/><strong>Listing 60-5:</strong> Example lines from <span class="literal">/etc/inetd.conf</span></p>
<p class="programsli">______________________________________________________________________<br/><br/># echo&#160;&#160;stream&#160;&#160;tcp&#160;&#160;nowait&#160;&#160;root&#160;&#160;&#160;&#160;internal<br/># echo&#160;&#160;dgram&#160;&#160;&#160;udp&#160;&#160;wait&#160;&#160;&#160;&#160;root&#160;&#160;&#160;&#160;internal<br/>ftp&#160;&#160;&#160;&#160;&#160;stream&#160;&#160;tcp&#160;&#160;nowait&#160;&#160;root&#160;&#160;&#160;&#160;/usr/sbin/tcpd&#160;&#160;&#160;in.ftpd<br/>telnet&#160;&#160;stream&#160;&#160;tcp&#160;&#160;nowait&#160;&#160;root&#160;&#160;&#160;&#160;/usr/sbin/tcpd&#160;&#160;&#160;in.telnetd<br/>login&#160;&#160;&#160;stream&#160;&#160;tcp&#160;&#160;nowait&#160;&#160;root&#160;&#160;&#160;&#160;/usr/sbin/tcpd&#160;&#160;&#160;in.rlogind<br/>______________________________________________________________________</p>
<p class="noindent">The first two lines of <a href="ch60.xhtml#ch60ex5">Listing 60-5</a> are commented out by the initial <span class="literal">#</span> character; we show them now since we&#8217;ll refer to the <em>echo</em> service shortly.</p>
<p class="indentb">Each line of <span class="literal">/etc/inetd.conf</span> consists of the following fields, delimited by white space:</p>
<p class="bull">&#8226; <em>Service name</em>: This specifies the name of a service from the <span class="literal">/etc/services</span> file. In conjunction with the <em>protocol</em> field, this is used to look up <span class="literal">/etc/services</span> to determine which port number <em>inetd</em> should monitor for this service.</p>
<p class="bull">&#8226; <em>Socket type</em>: This specifies the type of socket used by this service&#8212;for example, <span class="literal">stream</span> or <span class="literal">dgram</span>.</p>
<p class="bull">&#8226; <em>Protocol</em>: This specifies the protocol to be used by this socket. This field can contain any of the Internet protocols listed in the file <span class="literal">/etc/protocols</span> (documented in the <em>protocols(5)</em> manual page), but almost every service specifies either <span class="literal">tcp</span> (for TCP) or <span class="literal">udp</span> (for UDP).</p>
<p class="bull">&#8226; <em>Flags</em>: This field contains either <span class="literal">wait</span> or <span class="literal">nowait</span>. This field specifies whether or not the server execed by <em>inetd</em> (temporarily) takes over management of the socket for this service. If the execed server manages the socket, then this field is specified as <span class="literal">wait</span>. This causes <em>inetd</em> to remove this socket from the file descriptor set that it monitors using <em>select()</em> until the execed server exits (<em>inetd</em> detects this via a handler for <span class="literal">SIGCHLD</span>). We say some more about this field below.</p>
<p class="bull">&#8226; <em>Login name</em>: This field consists of a username from <span class="literal">/etc/passwd</span>, optionally followed by a period (<span class="literal">.</span>) and a group name from <span class="literal">/etc/group</span>. These determine the user and group IDs under which the execed server is run. (Since <em>inetd</em> runs with an effective user ID of <em>root</em>, its children are also privileged and can thus use calls to <em>setuid()</em> and <em>setgid()</em> to change process credentials if desired.)</p>
<p class="bull">&#8226; <em>Server program</em>: This specifies the pathname of the server program to be execed.</p>
<p class="bull">&#8226; <em>Server program arguments</em>: This field specifies one or more arguments, separated by white space, to be used as the argument list when execing the server program. The first of these corresponds to <em>argv[0]</em> in the execed program and is thus usually the same as the basename part of the <em>server program</em> name. The next argument corresponds to <em>argv[1]</em>, and so on.</p>
<div class="block">
<p class="noindent"><span epub:type="pagebreak" id="page_1250"/>In the example lines shown in <a href="ch60.xhtml#ch60ex5">Listing 60-5</a> for the <em>ftp</em>, <em>telnet</em>, and <em>login</em> services, we see the server program and arguments are set up differently than just described. All three of these services cause <em>inetd</em> to invoke the same program, <em>tcpd(8)</em> (the TCP daemon wrapper), which performs some logging and access-control checks before in turn execing the appropriate program, based on the value specified as the first server program argument (which is available to <em>tcpd</em> via <em>argv[0]</em>). Further information about <em>tcpd</em> can be found in the <em>tcpd(8)</em> manual page and in [<a href="bib.xhtml#bib61">Mann &#38; Mitchell, 2003</a>].</p>
</div>
<p class="noindent">Stream socket (TCP) servers invoked by <em>inetd</em> are normally designed to handle just a single client connection and then terminate, leaving <em>inetd</em> with the job of listening for further connections. For such servers, <em>flags</em> should be specified as <span class="literal">nowait</span>. (If, instead, the execed server is to accept connections, then <span class="literal">wait</span> should be specified, in which case <em>inetd</em> does not accept the connection, but instead passes the file descriptor for the <em>listening</em> socket to the execed server as descriptor 0.)</p>
<p class="indent">For most UDP servers, the <em>flags</em> field should be specified as <span class="literal">wait</span>. A UDP server invoked by <em>inetd</em> is normally designed to read and process all outstanding datagrams on the socket and then terminate. (This usually requires some sort of timeout when reading the socket, so that the server terminates when no new datagrams arrive within a specified interval.) By specifying <span class="literal">wait</span>, we prevent the <em>inetd</em> daemon from simultaneously trying to <em>select()</em> on the socket, which would have the unintended consequence that <em>inetd</em> would race the UDP server to check for datagrams and, if it won the race, start another instance of the UDP server.</p>
<div class="block">
<p class="noindent">Because the operation of <em>inetd</em> and the format of its configuration file are not specified by SUSv3, there are some (generally small) variations in the values that can be specified in the fields of <span class="literal">/etc/inetd.conf</span>. Most versions of <em>inetd</em> provide at least the syntax that we describe in the main text. For further details, see the <em>inetd.conf(8)</em> manual page.</p>
</div>
<p class="noindent">As an efficiency measure, <em>inetd</em> implements a few simple services itself, instead of execing separate servers to perform the task. The UDP and TCP <em>echo</em> services are examples of services that <em>inetd</em> implements. For such services, the <em>server program</em> field of the corresponding <span class="literal">/etc/inetd.conf</span> record is specified as <span class="literal">internal</span>, and the <em>server program arguments</em> are omitted. (In the example lines in <a href="ch60.xhtml#ch60ex5">Listing 60-5</a>, we saw that the <em>echo</em> service entries were commented out. To enable the <em>echo</em> service, we need to remove the <span class="literal">#</span> character at the start of these lines.)</p>
<p class="indent">Whenever we change the <span class="literal">/etc/inetd.conf</span> file, we need to send a <span class="literal">SIGHUP</span> signal to <em>inetd</em> to request it to reread the file:</p>
<p class="programs"># <span class="codestrong">killall -HUP inetd</span></p>
<h5 class="h5" id="ch60lev3sec06"><strong>Example: invoking a TCP <em>echo</em> service via <em>inetd</em></strong></h5>
<p class="noindenta">We noted earlier that <em>inetd</em> simplifies the programming of servers, especially concurrent (usually TCP) servers. It does this by carrying out the following steps on behalf of the servers it invokes:</p>
<ol>
<li class="order"><p class="orderp">Perform all socket-related initialization, calling <em>socket()</em>, <em>bind()</em>, and (for TCP servers) <em>listen()</em>.</p></li>
<li class="order"><p class="orderp">For a TCP service, perform an <em>accept()</em> for the new connection.</p></li>
<li class="order"><p class="orderp"><span epub:type="pagebreak" id="page_1251"/>Create a new process to handle the incoming UDP datagram or TCP connection. The process is automatically set up as a daemon. The <em>inetd</em> program handles all details of process creation via <em>fork()</em> and the reaping of dead children via a handler for <span class="literal">SIGCHLD</span>.</p></li>
<li class="order"><p class="orderp">Duplicate the file descriptor of the UDP socket or the connected TCP socket on file descriptors 0, 1, and 2, and close all other file descriptors (since they are unused in the execed server).</p></li>
<li class="order"><p class="orderp">Exec the server program.</p></li>
</ol>
<p class="noindent">(In the description of the above steps, we assume the usual cases that the <em>flags</em> field of the service entry in <span class="literal">/etc/inetd.conf</span> is specified as <span class="literal">nowait</span> for TCP services and <span class="literal">wait</span> for UDP services.)</p>
<p class="indent">As an example of how <em>inetd</em> simplifies the programming of a TCP service, in <a href="ch60.xhtml#ch60ex6">Listing 60-6</a>, we show the <em>inetd</em>-invoked equivalent of the TCP <em>echo</em> server from <a href="ch60.xhtml#ch60ex4">Listing 60-4</a>. Since <em>inetd</em> performs all of the above steps, all that remains of the server is the code executed by the child process to handle the client request, which can be read from file descriptor 0 (<span class="literal">STDIN_FILENO</span>).</p>
<p class="indent">If the server resides in the directory <span class="literal">/bin</span> (for example), then we would need to create the following entry in <span class="literal">/etc/inetd.conf</span> in order to have <em>inetd</em> invoke the server:</p>
<p class="programs">echo stream tcp nowait root /bin/is_echo_inetd_sv is_echo_inetd_sv</p>
<p class="examplet"><a id="ch60ex6"/><strong>Listing 60-6:</strong> TCP <em>echo</em> server designed to be invoked via <em>inetd</em></p>
<p class="programsli">________________________________________________ <span class="codestrong">sockets/is_echo_inetd_sv.c</span><br/><br/>#include &lt;syslog.h&gt;<br/>#include "tlpi_hdr.h"<br/><br/>#define BUF_SIZE 4096<br/><br/>int<br/>main(int argc, char *argv[])<br/>{<br/>&#160;&#160;&#160;&#160;char buf[BUF_SIZE];<br/>&#160;&#160;&#160;&#160;ssize_t numRead;<br/><br/>&#160;&#160;&#160;&#160;while ((numRead = read(STDIN_FILENO, buf, BUF_SIZE)) &gt; 0) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;if (write(STDOUT_FILENO, buf, numRead) != numRead) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "write() failed: %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;}<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;if (numRead == -1) {<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;syslog(LOG_ERR, "Error from read(): %s", strerror(errno));<br/>&#160;&#160;&#160;&#160;&#160;&#160;&#160;&#160;exit(EXIT_FAILURE);<br/>&#160;&#160;&#160;&#160;}<br/><br/>&#160;&#160;&#160;&#160;exit(EXIT_SUCCESS);<br/>}<br/>________________________________________________ <span class="codestrong">sockets/is_echo_inetd_sv.c</span></p>
<h3 class="h3" id="ch60lev1sec06"><span epub:type="pagebreak" id="page_1252"/><strong>60.6 Summary</strong></h3>
<p class="noindenta">An iterative server handles one client at a time, processing that client&#8217;s request(s) completely, before proceeding to the next client. A concurrent server handles multiple clients simultaneously. In high-load scenarios, a traditional concurrent server design that creates a new child process (or thread) for each client may not perform well enough, and we outlined a range of other approaches for concurrently handling large numbers of clients.</p>
<p class="indent">The Internet superserver daemon, <em>inetd</em>, monitors multiple sockets and starts the appropriate servers in response to incoming UDP datagrams or TCP connections. Using <em>inetd</em> allows us to decrease system load by minimizing the number of network server processes on the system, and also simplifies the programming of server processes, since it performs most of the initialization steps required by a server.</p>
<h5 class="h5" id="ch60lev3sec07"><strong>Further information</strong></h5>
<p class="noindenta">Refer to the sources of further information listed in <a href="ch59.xhtml#ch59lev1sec15">Section 59.15</a>.</p>
<h3 class="h3" id="ch60lev1sec07"><strong>60.7 Exercises</strong></h3>
<p class="exer"><a id="ch60exe1"/><strong>60-1.</strong>&#160;&#160;&#160;Add code to the program in <a href="ch60.xhtml#ch60ex4">Listing 60-4</a> (<span class="literal">is_echo_sv.c</span>) to place a limit on the number of simultaneously executing children.</p>
<p class="exer"><a id="ch60exe2"/><strong>60-2.</strong>&#160;&#160;&#160;Sometimes, it may be necessary to write a socket server so that it can be invoked either directly from the command line or indirectly via <em>inetd</em>. In this case, a command-line option is used to distinguish the two cases. Modify the program in <a href="ch60.xhtml#ch60ex4">Listing 60-4</a> so that, if it is given a <em>&#8211;i</em> command-line option, it assumes that it is being invoked by <em>inetd</em> and handles a single client on the connected socket, which <em>inetd</em> supplies via <span class="literal">STDIN_FILENO</span>. If the <em>&#8211;i</em> option is not supplied, then the program can assume it is being invoked from the command line, and operate in the usual fashion. (This change requires only the addition of a few lines of code.) Modify <span class="literal">/etc/inetd.conf</span> to invoke this program for the <em>echo</em> service.</p>
</body>
</html>
